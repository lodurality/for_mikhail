{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites: sklearn, pandas, numpy, scipy, ipython/jupyter, xgboost, matplotlib/seaborn libraries for python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data aquisition\n",
    "\n",
    "Get the train data from 'Walmart Recruiting: Trip Type Classification' competition:\n",
    "https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/data\n",
    "\n",
    "Note 1: you can also pass this assignment if you will provide notebook with 0.55 log loss or better result on 10-fold cross-validation. \n",
    "\n",
    "Note 2: if this competition is your project, consider this assignment as first steps in it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give summary of train features. \n",
    "\n",
    "— Which of them are numerical and categorical? \n",
    "\n",
    "— What distributions do they have? Are distributions identical on test and train? \n",
    "\n",
    "— What features contain missing values? What's you guess why?\n",
    "\n",
    "Note: Pandas library can be very handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Missing values. How do you choose to impute them? Why?\n",
    "\n",
    "— String variables. Implement label and one-hot encoding. For what models it makes a difference? (Note: it may be better to use Pandas get_dummies method to one-hot encoder in sklearn).\n",
    "\n",
    "— Combine observations by VisitNumber (Pandas groupby method can be very handy in that case). Make feature vector for each VisitNumber.  \n",
    "\n",
    "— Split train data into even new_train and evaluation sets using train_test_split from sklearn. Fix random state for reproducibility of your results. \n",
    "\n",
    "— Choose cross-validation 5-fold split for your data (3-fold if your PC is not so fast). Do not forget to specify random state for your results reproducibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNN-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Try KNN classifier on the obtained features. Report accuracy and logarithmic loss (mean and std across the folds). How does this evaluation metrics change depending on neighbors number? Depending on distance metric? \n",
    "\n",
    "— Show normalized confusion matrix for the best obtained model performance on the evaluation set. How does it differ from cross-validation score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Try logistic regression and linear SVM on your features. Report accuracy and log loss. Compare it to KNN-classifier. How does performance differ whether you use data scaling or not? How does performance differ if you use on-hot or label encoded categorical features?\n",
    "\n",
    "— Regularization. Play with l1 and l2 regularization parameters. Plot log loss score depending on different C values for l1 and l2 regularizations. What regualarization do you choose with which parameter? Why? \n",
    "\n",
    "— Show normalized confusion matrix for the best obtained model performance on the evaluation set. How does it differ from cross-validation score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Try vanilla Random Forest, AdaBoost, GBDT from sklearn and GBDT from xgboost. Report cross-validation log loss (mean and std) for each of these models.\n",
    "\n",
    "— Random Forest. Find best parameters for the Random Forest using grid search on cross-validation. What are the most important? Report result on evaluation test. \n",
    "\n",
    "— Adaboost. Find best parameters for the AdaBoost using grid search on cross-validation. What are the most important? Plot learning curve (log loss on each conscutive iteration) for the best AdaBoost on new_train and evaluation sets. Note: staged_predict_proba method can be very useful. \n",
    "\n",
    "— GBDT. Compare speed and performance of sklearn/xgboost GBDT implementation with similar parameters. What implementation do your choose? Do not forget to utilise all of your CPUs/threads to speed up calculations. Find best parameters for the GBDT using grid search on cross-validation. What are the most important? Plot learning curve (log loss on each conscutive iteration) for the best GBDT on new_train and evaluation sets. Note: staged_predict_proba method can be very useful for sklearn and evals_result method for xgboost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model comaprison and combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Compare performance of all used models in terms of accuracy and log loss on evaluation. What is the best single model performance?\n",
    "\n",
    "— (optional) Try to imporve your single model performnace with weighted combination of other models predictions. Note: sklearn VotingClassifier with 'softmax' option can be useful. What is the best model performance on evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
